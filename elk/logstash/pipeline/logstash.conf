# Logstash Pipeline Configuration for AI Study Circle
# Main pipeline for processing application logs

input {
  # Filebeat input for log files
  beats {
    port => 5044
    host => "0.0.0.0"
  }
  
  # HTTP input for direct log submissions from frontend
  http {
    port => 8080
    host => "0.0.0.0"
    codec => json
    additional_codecs => {
      "application/json" => "json"
    }
  }
  
  # TCP input for backend Winston logs
  tcp {
    port => 5000
    host => "0.0.0.0"
    codec => json_lines
  }
  
  # File input for backend log files
  file {
    path => "/logs/backend/*.log"
    start_position => "beginning"
    codec => json
    tags => ["backend", "winston"]
  }
  
  # File input for frontend log files (if any)
  file {
    path => "/logs/frontend/*.json"
    start_position => "beginning"
    codec => json
    tags => ["frontend"]
  }
}

filter {
  # Load custom patterns
  grok {
    patterns_dir => ["/usr/share/logstash/pipeline/../patterns"]
  }
  
  # Add processing timestamp and metadata
  mutate {
    add_field => { 
      "[@metadata][processed_at]" => "%{+YYYY-MM-dd'T'HH:mm:ss.SSSZ}"
      "[@metadata][pipeline_version]" => "2.0"
      "[@metadata][enhanced_processing]" => "true"
    }
  }
  
  # Handle different log sources
  if [agent][type] == "filebeat" {
    # Filebeat logs
    mutate {
      add_tag => ["filebeat"]
    }
    
    # Parse log file path to determine service
    grok {
      match => { "[log][file][path]" => ".*/logs/(?<service>backend|frontend)/.*" }
      add_field => { "source" => "%{service}" }
    }
  } else if "backend" in [tags] {
    # Backend Winston logs
    mutate {
      add_field => { "source" => "backend" }
      add_field => { "service" => "ai-study-backend" }
    }
  } else if "frontend" in [tags] {
    # Frontend logs
    mutate {
      add_field => { "source" => "frontend" }
      add_field => { "service" => "ai-study-frontend" }
    }
  } else {
    # Direct HTTP/TCP input
    if [source] {
      # Source already specified in log
      mutate {
        add_field => { "service" => "ai-study-%{source}" }
      }
    } else {
      # Default to backend for unspecified sources
      mutate {
        add_field => { "source" => "backend" }
        add_field => { "service" => "ai-study-backend" }
      }
    }
  }
  
  # Normalize timestamp field
  if [timestamp] {
    date {
      match => [ "timestamp", "ISO8601" ]
      target => "@timestamp"
    }
  }
  
  # Parse user agent (if present)
  if [context][deviceInfo][userAgent] or [data][userAgent] or [userAgent] {
    useragent {
      source => "[context][deviceInfo][userAgent]"
      target => "user_agent"
    }
    
    if ![user_agent] and [data][userAgent] {
      useragent {
        source => "[data][userAgent]"
        target => "user_agent"
      }
    }
    
    if ![user_agent] and [userAgent] {
      useragent {
        source => "userAgent"
        target => "user_agent"
      }
    }
  }
  
  # Extract HTTP information
  if [context][url] or [data][url] or [url] {
    mutate {
      add_field => { "[http][url]" => "%{[context][url]}" }
    }
    
    # Parse URL components
    grok {
      match => { "[http][url]" => "^(?<[http][protocol]>https?://)(?<[http][host]>[^:/]+)(?::(?<[http][port]>\d+))?(?<[http][path]>/[^?]*)?(?:\?(?<[http][query]>.*))?$" }
    }
  }
  
  # Handle method field
  if [data][method] {
    mutate {
      add_field => { "[http][method]" => "%{[data][method]}" }
    }
  }
  
  # Handle status code
  if [data][status] or [data][statusCode] {
    mutate {
      add_field => { "[http][status_code]" => "%{[data][status]}" }
    }
    if ![http][status_code] and [data][statusCode] {
      mutate {
        add_field => { "[http][status_code]" => "%{[data][statusCode]}" }
      }
    }
  }
  
  # Handle response time
  if [data][duration] or [data][responseTime] {
    mutate {
      add_field => { "[http][response_time]" => "%{[data][duration]}" }
    }
    if ![http][response_time] and [data][responseTime] {
      mutate {
        add_field => { "[http][response_time]" => "%{[data][responseTime]}" }
      }
    }
  }
  
  # Extract error information
  if [level] == "error" {
    if [data][name] {
      mutate {
        add_field => { "[error][name]" => "%{[data][name]}" }
      }
    }
    if [data][stack] {
      mutate {
        add_field => { "[error][stack]" => "%{[data][stack]}" }
      }
    }
    if [data][code] {
      mutate {
        add_field => { "[error][code]" => "%{[data][code]}" }
      }
    }
  }
  
  # Extract performance information
  if [category] == "performance" or [context][category] == "performance" {
    if [data][metric] {
      mutate {
        add_field => { "[performance][metric]" => "%{[data][metric]}" }
      }
    }
    if [data][value] {
      mutate {
        add_field => { "[performance][value]" => "%{[data][value]}" }
      }
    }
    if [data][threshold] {
      mutate {
        add_field => { "[performance][threshold]" => "%{[data][threshold]}" }
      }
    }
  }
  
  # Extract user action information
  if [category] == "user_action" or [context][category] == "user_action" {
    if [data][action] {
      mutate {
        add_field => { "[user_action][action]" => "%{[data][action]}" }
      }
    }
    if [data][element] {
      mutate {
        add_field => { "[user_action][element]" => "%{[data][element]}" }
      }
    }
    if [data][coordinates] {
      mutate {
        add_field => { "[user_action][coordinates]" => "%{[data][coordinates]}" }
      }
    }
  }
  
  # Extract API information
  if [category] == "api" or [context][category] == "api" {
    if [data][endpoint] or [data][url] {
      mutate {
        add_field => { "[api][endpoint]" => "%{[data][endpoint]}" }
      }
      if ![api][endpoint] and [data][url] {
        mutate {
          add_field => { "[api][endpoint]" => "%{[data][url]}" }
        }
      }
    }
  }
  
  # Session and user information
  if [context][sessionId] or [sessionId] {
    mutate {
      add_field => { "sessionId" => "%{[context][sessionId]}" }
    }
    if ![sessionId] and [sessionId] {
      # sessionId is already at root level, keep it
    }
  }
  
  if [context][userId] or [userId] {
    mutate {
      add_field => { "userId" => "%{[context][userId]}" }
    }
  }
  
  # Add environment information
  if ![environment] {
    if [source] == "frontend" {
      mutate {
        add_field => { "environment" => "browser" }
      }
    } else {
      mutate {
        add_field => { "environment" => "server" }
      }
    }
  }
  
  # Convert numeric fields to proper types
  mutate {
    convert => { 
      "[http][status_code]" => "integer"
      "[http][response_time]" => "integer"
      "[performance][value]" => "float"
      "[user_action][coordinates][x]" => "integer"
      "[user_action][coordinates][y]" => "integer"
    }
  }
  
  # Enhanced processing with custom patterns
  if [message] and [message] !~ /^\{.*\}$/ {
    # Try to parse non-JSON log messages with custom patterns
    grok {
      match => { 
        "message" => [
          "%{AISTUDYREQUEST}",
          "%{AISTUDYAUTH}",
          "%{AISTUDYERROR_LOG}",
          "%{AISTUDYPERFORMANCE}",
          "%{AISTUDYUSER_ACTION}",
          "%{AISTUDYAPI_CALL}",
          "%{AISTUDYTRANSLATE_LOG}",
          "%{AISTUDYAI_LOG}",
          "%{AISTUDYSECURITY_LOG}",
          "%{AISTUDYBACKEND_LOG}",
          "%{AISTUDYFRONTEND_LOG}"
        ]
      }
      tag_on_failure => ["_grokparsefailure"]
    }
  }
  
  # GeoIP enrichment for client IPs
  if [client_ip] and [client_ip] != "127.0.0.1" and [client_ip] !~ /^192\.168\./ and [client_ip] !~ /^10\./ {
    geoip {
      source => "client_ip"
      target => "geoip"
    }
  }
  
  # Enhanced field enrichment
  ruby {
    code => "
      # Performance score calculation
      if event.get('[performance]')
        lcp = event.get('[performance][lcp]').to_f rescue nil
        fid = event.get('[performance][fid]').to_f rescue nil
        cls = event.get('[performance][cls]').to_f rescue nil
        
        if lcp && fid && cls
          # Calculate Web Vitals score (0-100)
          lcp_score = lcp <= 2500 ? 100 : (lcp <= 4000 ? 50 : 0)
          fid_score = fid <= 100 ? 100 : (fid <= 300 ? 50 : 0)
          cls_score = cls <= 0.1 ? 100 : (cls <= 0.25 ? 50 : 0)
          
          overall_score = (lcp_score + fid_score + cls_score) / 3
          event.set('[performance][web_vitals_score]', overall_score.round)
          
          if overall_score >= 80
            event.set('[performance][grade]', 'good')
          elsif overall_score >= 50
            event.set('[performance][grade]', 'needs_improvement')
          else
            event.set('[performance][grade]', 'poor')
          end
        end
      end
      
      # Risk scoring for security events
      if event.get('[security_event_type]')
        risk_score = 0
        
        # Base risk by event type
        case event.get('[security_event_type]')
        when 'authentication'
          risk_score += event.get('[auth_result]') == 'failure' ? 3 : 1
        when 'authorization'
          risk_score += 5
        when 'suspicious_activity'
          risk_score += 7
        when 'rate_limiting'
          risk_score += 2
        end
        
        # IP reputation (simplified)
        if event.get('[geoip][country_name]')
          # Higher risk for certain regions (simplified example)
          high_risk_countries = ['CN', 'RU', 'KP']
          if high_risk_countries.include?(event.get('[geoip][country_code2]'))
            risk_score += 2
          end
        end
        
        # Status code risk
        status = event.get('[http][status_code]').to_i rescue 0
        if status == 401 || status == 403
          risk_score += 2
        elsif status >= 500
          risk_score += 1
        end
        
        event.set('[security][risk_score]', risk_score)
        
        if risk_score >= 8
          event.set('[security][risk_level]', 'critical')
        elsif risk_score >= 5
          event.set('[security][risk_level]', 'high')
        elsif risk_score >= 2
          event.set('[security][risk_level]', 'medium')
        else
          event.set('[security][risk_level]', 'low')
        end
      end
      
      # Session duration calculation
      if event.get('[session_event]') == 'end' && event.get('[session_duration]')
        duration_ms = event.get('[session_duration]').to_i rescue 0
        if duration_ms > 0
          duration_minutes = duration_ms / 60000.0
          event.set('[session][duration_minutes]', duration_minutes.round(2))
          
          if duration_minutes < 1
            event.set('[session][engagement]', 'bounce')
          elsif duration_minutes < 5
            event.set('[session][engagement]', 'short')
          elsif duration_minutes < 15
            event.set('[session][engagement]', 'medium')
          else
            event.set('[session][engagement]', 'long')
          end
        end
      end
      
      # Add processing metadata
      event.set('[@metadata][enriched]', true)
      event.set('[@metadata][processing_version]', '2.0')
    "
  }
  
  # Additional categorization and tagging
  if [log_type] == "performance" {
    mutate {
      add_tag => ["performance_log"]
    }
    
    if [performance][grade] == "poor" {
      mutate {
        add_tag => ["performance_issue", "needs_optimization"]
      }
    }
  }
  
  if [log_type] == "security" {
    mutate {
      add_tag => ["security_log"]
    }
    
    if [security][risk_level] == "critical" or [security][risk_level] == "high" {
      mutate {
        add_tag => ["security_alert", "investigate"]
      }
    }
  }
  
  if [log_type] == "user_activity" {
    mutate {
      add_tag => ["user_activity_log"]
    }
    
    if [engagement_score] and [engagement_score] >= 5 {
      mutate {
        add_tag => ["high_engagement"]
      }
    }
  }
  
  # Clean up fields
  mutate {
    remove_field => [ 
      "host", 
      "agent", 
      "ecs",
      "input",
      "log",
      "@version"
    ]
  }
  
  # Add tags based on conditions
  if [level] == "error" {
    mutate {
      add_tag => ["error", "needs-attention"]
    }
  }
  
  if [http][status_code] >= 400 {
    mutate {
      add_tag => ["http-error"]
    }
  }
  
  if [http][status_code] >= 500 {
    mutate {
      add_tag => ["server-error", "critical"]
    }
  }
  
  if [http][response_time] > 3000 {
    mutate {
      add_tag => ["slow-response", "performance-issue"]
    }
  }
  
  if [performance][threshold] == "poor" {
    mutate {
      add_tag => ["performance-issue", "needs-optimization"]
    }
  }
}

output {
  # Send to Elasticsearch
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "ai-study-logs-%{+YYYY.MM.dd}"
    template_name => "ai-study-logs"
    template => "/usr/share/logstash/config/elasticsearch-template.json"
    template_overwrite => true
  }
  
  # Debug output (remove in production)
  if [level] == "debug" {
    stdout {
      codec => rubydebug
    }
  }
  
  # Error output for failed processing
  if "_grokparsefailure" in [tags] {
    file {
      path => "/usr/share/logstash/logs/failed-grok.log"
      codec => json
    }
  }
}